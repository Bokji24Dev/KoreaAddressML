{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.2.2 tqdm pandas torch transformers sentencepiece numpy==1.23.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 코드\n",
    "\n",
    "`archive_v3.zip` 파일에 최종본을 저장했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_string(s):\n",
    "    # 문자열을 공백을 기준으로 분리\n",
    "    words = s.split()\n",
    "    \n",
    "    # 단어의 개수가 1개 이하인 경우 변환 없이 반환\n",
    "    if len(words) <= 1:\n",
    "        return s\n",
    "    \n",
    "    # 마지막 두 단어가 같은 경우\n",
    "    if words[-1] == words[-2]:\n",
    "        # 마지막 단어를 제외하고 나머지를 재조합\n",
    "        return ' '.join(words[:-1])\n",
    "    else:\n",
    "        # 두 단어가 같지 않다면 원래 문자열 반환\n",
    "        return s\n",
    "\n",
    "def generate_number():\n",
    "    for first_part in range(1, 999 + 1):\n",
    "        for second_part in range(0, 99 + 1):\n",
    "            if second_part == 0:\n",
    "                yield f\"{first_part}\"\n",
    "            else:\n",
    "                yield f\"{first_part}-{second_part}\"\n",
    "\n",
    "root_path = \"dataset/archive/\"\n",
    "files = list(map(lambda x: root_path + x, os.listdir(root_path)))\n",
    "rows = []\n",
    "for file in tqdm(files):\n",
    "    with open(file, \"rb\") as f:\n",
    "        items = pickle.load(f)\n",
    "        rows.extend(list(map(lambda x: process_string(x[\"text\"]) + \"\\n\", items)))\n",
    "\n",
    "# 생성된 숫자 출력\n",
    "for item in generate_number():\n",
    "    rows.append(item + \"\\n\")\n",
    "\n",
    "with open(\"dataset/koraddr.txt\", \"w\") as f:\n",
    "    f.writelines(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE 토크나이저 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from transformers import LlamaTokenizerFast\n",
    "\n",
    "input_file = 'dataset/koraddr.txt'\n",
    "model_prefix = 'tokenizer'\n",
    "model_type = 'bpe'\n",
    "vocab_size = 30000\n",
    "\n",
    "# 센텐스피스 트레이너를 사용하여 모델 훈련\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={input_file} --model_prefix={model_prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=512\" + # 문장 최대 길이 -> 이게 너무 길면 에러발생함\n",
    "    \" --pad_id=0 --pad_piece=<pad>\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=<unk>\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=<s>\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=</s>\" + # end of sequence (3)\n",
    "    \" --byte_fallback=true\" + # add byte_fallback for unk tokens\n",
    "    \" --user_defined_symbols=<sep>,<cls>,<mask>\" # 사용자 정의 토큰\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizerFast(vocab_file=f\"{model_prefix}.model\")\n",
    "tokenizer.save_pretrained(\"KoreaAddressTokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "blank_token = 29528\n",
    "\n",
    "def pad_list_to_32(data: List[int], padding_value=blank_token):\n",
    "    # 리스트의 길이가 32보다 작은 동안 29528를 추가\n",
    "    while len(data) < 32:\n",
    "        data.append(padding_value)\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_save(global_rows, idx, folder):\n",
    "    num_files = 256\n",
    "    # 분류(class)별로 데이터 나누기\n",
    "    class_data = defaultdict(list)\n",
    "    for row in global_rows:\n",
    "        class_data[row['class']].append(row)\n",
    "\n",
    "    # 데이터 섞기\n",
    "    for key in class_data:\n",
    "        random.shuffle(class_data[key])\n",
    "\n",
    "    # 모든 데이터를 합치기\n",
    "    all_data = []\n",
    "    for key in class_data:\n",
    "        all_data.extend(class_data[key])\n",
    "\n",
    "    # 데이터를 num_files개의 파일로 나누기\n",
    "    chunk_size = len(all_data) // num_files\n",
    "    chunks = [all_data[i * chunk_size:(i + 1) * chunk_size] for i in range(num_files)]\n",
    "\n",
    "    # 마지막 chunk에 남은 데이터 추가\n",
    "    if len(all_data) % num_files != 0:\n",
    "        chunks[-1].extend(all_data[num_files * chunk_size:])\n",
    "\n",
    "    root_path = f'dataset/koraddr_dataset/{folder}/' + str(idx)\n",
    "    # 디렉토리 생성\n",
    "    if not os.path.exists(root_path):\n",
    "        os.makedirs(root_path)\n",
    "\n",
    "    # pickle 파일로 저장\n",
    "    for i, chunk in tqdm(enumerate(chunks), total=len(chunks), position=1):\n",
    "        with open(f'{root_path}/data_chunk_{i + 1}.pk', 'wb') as file:\n",
    "            pickle.dump(chunk, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE 토크나이저로 주소데이터 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 허깅페이스에 모델을 공개해둔 상태입니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Mineru/KoreaAddressTokenizer\")\n",
    "\n",
    "root_path = \"dataset/archive/\"\n",
    "files = list(map(lambda x: root_path + x, os.listdir(root_path)))\n",
    "\n",
    "for i, file in tqdm(enumerate(files), total=len(files), position=0):\n",
    "    with open(file, \"rb\") as f:\n",
    "        rows = pickle.load(f)\n",
    "\n",
    "    check_sum = True\n",
    "    idx = 0\n",
    "    texts = []\n",
    "    for j, row in enumerate(rows):\n",
    "        texts.append(row[\"text\"])\n",
    "        if check_sum:\n",
    "            idx = j\n",
    "            check_sum = False\n",
    "        if len(texts) == 1024:\n",
    "            tokens = tokenizer.batch_encode_plus(texts)\n",
    "            for k, ids in enumerate(tokens['input_ids']):\n",
    "                rows[idx + k][\"input_ids\"] = pad_list_to_32(ids)\n",
    "            check_sum = True\n",
    "            texts = []\n",
    "    tokens = tokenizer.batch_encode_plus(texts)\n",
    "    for j, ids in enumerate(tokens['input_ids']):\n",
    "        rows[idx + j][\"input_ids\"] = ids\n",
    "    \n",
    "    with open(root_path + file.split(\"/\")[-1], 'wb') as f:\n",
    "        pickle.dump(rows, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"dataset/archive/\"\n",
    "files = {}\n",
    "\n",
    "for file_path in os.listdir(root_path):\n",
    "    location = file_path.split(\"_\")[0]\n",
    "    if location not in files.keys():\n",
    "        files[location] = []\n",
    "    idx = file_path.replace(location + \"_\", \"\").split(\".\")[0]\n",
    "    files[location].append({\n",
    "        \"idx\": int(idx),\n",
    "        \"location\": location,\n",
    "        \"file\": root_path + file_path\n",
    "    })\n",
    "\n",
    "for location in files.keys():\n",
    "    files[location] = sorted(files[location], key=lambda x: x['idx'])\n",
    "\n",
    "root_path = \"dataset/archive_v2/\"\n",
    "if not os.path.exists(root_path):\n",
    "    os.makedirs(root_path)\n",
    "for location in files.keys():\n",
    "    rows = []\n",
    "    for item in tqdm(files[location], position=0):\n",
    "        with open(item[\"file\"], \"rb\") as f:\n",
    "            rows.extend(pickle.load(f))\n",
    "    with open(f'{root_path}{files[location][0][\"location\"]}.pk', 'wb') as f:\n",
    "        pickle.dump(rows, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"dataset/archive_v2/\"\n",
    "files = list(map(lambda x: root_path + x, os.listdir(root_path)))\n",
    "\n",
    "root_path = \"dataset/archive_v3/\"\n",
    "if not os.path.exists(root_path):\n",
    "    os.makedirs(root_path)\n",
    "for file in tqdm(files, position=0):\n",
    "    with open(file, \"rb\") as f:\n",
    "        rows = pickle.load(f)\n",
    "    split_save(rows, file.split(\"/\")[-1].split(\".\")[0], \"archive_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 초기화(여기서 부터 실행하면 됩니다.)\n",
    "\n",
    "`archive_v3.zip` 파일을 압축을 dataset 아래에 푼 다음 진행해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 랜덤 생성 시드 고정\n",
    "seed = 42\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "blank_token = 12235 # 빈 문자열 토큰 정의\n",
    "root_path = \"dataset/archive_v3/\" # 데이터셋\n",
    "\n",
    "# 모든 데이터가 아닌 지역별로 256갱\n",
    "files = [[] for _ in range(256)]\n",
    "for location in os.listdir(root_path):\n",
    "    for i in range(1, 256 + 1):\n",
    "        files[i - 1].append(root_path + location + \"/data_chunk_\" + str(i) + \".pk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 학습 모델 정의\n",
    "# Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=seed)\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(\n",
    "    solver=\"lbfgs\", random_state=seed, max_iter=50000, C=0.1, multi_class=\"multinomial\"\n",
    ")\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=1000, random_state=seed)\n",
    "#  Naive Bayes\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# K-NN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=17, n_jobs=os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변환 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "def load_data(filepaths: List[str]):\n",
    "    rows = []\n",
    "    # 피클 파일 로드\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            rows.extend(pickle.load(f))\n",
    "    random.shuffle(rows)\n",
    "    df = pd.DataFrame(rows)\n",
    "    # input_ids가 현재 List[int] 형태로 되어있는데 해당 형식을 각각 칼럼으로 만들고 Int64로 형 변환 후 데이터프레임으로 저장\n",
    "    input_ids_df = pd.DataFrame(\n",
    "        df[\"input_ids\"].tolist(), columns=[f\"input_ids_{i+1}\" for i in range(32)]\n",
    "    ).astype(\"Int64\")\n",
    "    # 기존의 DataFrame에 새롭게 만든 input_ids DataFrame을 합치고 기존에 List[int]로 된 input_ids 칼럼을 제거\n",
    "    df = pd.concat([df, input_ids_df], axis=1).drop(columns=[\"input_ids\"])\n",
    "    # input_ids_1부터 input_ids_32 까지의 칼럼만 Feature로 남겨두기\n",
    "    X = df.drop(columns=[\"text\", \"class\"])\n",
    "    # class 칼럼만 정답 데이터로 남겨두기\n",
    "    y = df[\"class\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(\n",
    "    modelName: str,\n",
    "    i,\n",
    "    X_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    y_test: pd.DataFrame,\n",
    "):\n",
    "    print(f\"{modelName} Train\")\n",
    "    if modelName == \"dt\":\n",
    "        dt_model.fit(X_train, y_train)\n",
    "        model = dt_model\n",
    "    elif modelName == \"rf\":\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        model = rf_model\n",
    "    elif modelName == \"lr\":\n",
    "        lr_model.fit(X_train, y_train)\n",
    "        model = lr_model\n",
    "    elif modelName == \"gnb\":\n",
    "        gnb_model.fit(X_train, y_train)\n",
    "        model = gnb_model\n",
    "    elif modelName == \"knn\":\n",
    "        knn_model.fit(X_train, y_train)\n",
    "        model = knn_model\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"name\": modelName,\n",
    "            \"train/step\": i + 1,\n",
    "            \"train/accuracy\": accuracy,\n",
    "        }\n",
    "    )\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개별 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wandb.init(project=\"korea-address-classification\", entity=\"mineru\")\n",
    "\n",
    "# 글로벌 변수\n",
    "g_x_test, g_y_test = pd.DataFrame(),  pd.DataFrame()\n",
    "\n",
    "modelname = \"dt\"\n",
    "\n",
    "for i, file in tqdm(enumerate(files), total=len(files), position=0):\n",
    "    X, y = load_data(file)\n",
    "    X = X.dropna()\n",
    "    y = y[X.index]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed\n",
    "    )\n",
    "    g_x_test = pd.concat([g_x_test, X_test], ignore_index=True)\n",
    "    g_y_test = pd.concat([g_y_test, y_test], ignore_index=True)\n",
    "    train(modelname, i, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    if i != 0 and i % 50 == 0:\n",
    "        # 모델 체크포인트 저장\n",
    "        if modelname == \"dt\":\n",
    "            joblib.dump(\n",
    "                dt_model,\n",
    "                f\"models/dt_model.{i}.joblib\",\n",
    "            )\n",
    "        elif modelname == \"lr\":\n",
    "            joblib.dump(\n",
    "                lr_model,\n",
    "                f\"models/lr_model.{i}.joblib\",\n",
    "            )\n",
    "        elif modelname == \"rf\":\n",
    "            joblib.dump(\n",
    "                rf_model,\n",
    "                f\"models/rf_model.{i}.joblib\",\n",
    "            )\n",
    "        elif modelname == \"gnb\":\n",
    "            joblib.dump(\n",
    "                gnb_model,\n",
    "                f\"models/gnb_model.{i}.joblib\",\n",
    "            )\n",
    "        elif modelname == \"knn\":\n",
    "            joblib.dump(\n",
    "                knn_model,\n",
    "                f\"models/knn_model.{i}.joblib\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개별 학습 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelname == \"dt\":\n",
    "    joblib.dump(\n",
    "        dt_model, f\"models/dt_model_v1.joblib\"\n",
    "    )\n",
    "elif modelname == \"lr\":\n",
    "    joblib.dump(\n",
    "        lr_model, f\"models/lr_model_v1.joblib\"\n",
    "    )\n",
    "elif modelname == \"rf\":\n",
    "    joblib.dump(\n",
    "        rf_model, f\"models/rf_model_v1.joblib\"\n",
    "    )\n",
    "elif modelname == \"gnb\":\n",
    "    joblib.dump(\n",
    "        gnb_model, f\"models/gnb_model_v1.joblib\"\n",
    "    )\n",
    "elif modelname == \"knn\":\n",
    "    joblib.dump(\n",
    "        knn_model, f\"models/knn_model_v1.joblib\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:30<00:00,  5.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 랜덤 생성 시드 고정\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "\n",
    "models = [\n",
    "    joblib.load(\"models/dt_model_v1.joblib\"),\n",
    "    joblib.load(\"models/rf_model_v1.joblib\"),\n",
    "    joblib.load(\"models/knn_model_v1.joblib\"),\n",
    "]\n",
    "\n",
    "g_x_train, g_y_train = pd.DataFrame(),  pd.DataFrame()\n",
    "g_x_test, g_y_test = pd.DataFrame(),  pd.DataFrame()\n",
    "\n",
    "root_path = \"dataset/archive_v3/\"\n",
    "files = [[] for _ in range(256)]\n",
    "for location in os.listdir(root_path):\n",
    "    for i in range(1, 256 + 1):\n",
    "        files[i - 1].append(root_path + location + \"/data_chunk_\" + str(i) + \".pk\")\n",
    "\n",
    "files = files[:17 * 4] # 일부 데이터만 로드\n",
    "random.shuffle(files)\n",
    "\n",
    "for i, file in tqdm(enumerate(files), total=len(files)):\n",
    "    X, y = load_data(file)\n",
    "    X = X.dropna()\n",
    "    y = y[X.index]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed\n",
    "    )\n",
    "    g_x_train = pd.concat([g_x_train, X_train], ignore_index=True)\n",
    "    g_y_train = pd.concat([g_y_train, y_train], ignore_index=True)\n",
    "    g_x_test = pd.concat([g_x_test, X_test], ignore_index=True)\n",
    "    g_y_test = pd.concat([g_y_test, y_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[(\"dt\", models[0]), (\"rf\", models[1]), (\"knn\", models[2])],\n",
    "    voting=\"soft\",\n",
    "    weights=[6, 3, 1],\n",
    ")\n",
    "\n",
    "# 배치 학습 비교를 위해 배치 사이즈 분할\n",
    "for batch_size in [1000, 100000, 1000000, 10000000]:\n",
    "    with tqdm(total=len(g_x_train), desc=\"Training\") as pbar:\n",
    "        for i in range(0, len(g_x_train), batch_size):\n",
    "            end = i + batch_size if i + batch_size <= len(g_x_train) else len(g_x_train)\n",
    "            X_batch = g_x_train[i:end]\n",
    "            y_batch = g_y_train[i:end]\n",
    "            ensemble_model.fit(X_batch, y_batch)  # 배치 단위로 학습\n",
    "            pbar.update(end - i)\n",
    "    ensemble_pred = ensemble_model.predict(g_x_test)\n",
    "    ensemble_acc = accuracy_score(g_y_test, ensemble_pred)\n",
    "    print(f\"Ensemble Accuracy: {ensemble_acc:.4f}\")\n",
    "    joblib.dump(ensemble_model, f\"models/ensemble_model_v1.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "num_classes = 17\n",
    "batch_size = 256\n",
    "\n",
    "# CNN 모델 정의\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 1 * 1, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.pool = nn.MaxPool2d((1, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 1 * 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# 모델 초기화\n",
    "model = SimpleCNN()\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001) # 최적화 함수 정의\n",
    "running_loss = 0.0\n",
    "\n",
    "# 학습 횟수\n",
    "num_epochs = 20\n",
    "\n",
    "PATH = \"./models/cnn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "result = []\n",
    "accuracis = []\n",
    "\n",
    "for i, file in tqdm(enumerate(files), total=len(files), position=0):\n",
    "    X, y = load_data(file)\n",
    "    X = X.dropna()\n",
    "    y = y[X.index]\n",
    "\n",
    "    # pandas DataFrame을 Tensor로 변환\n",
    "    X_tensor = torch.tensor(X.values, dtype=torch.float32).unsqueeze(2).unsqueeze(3)\n",
    "    y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), position=1):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # 데이터도 GPU로 이동\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        result.append(\n",
    "            {\n",
    "                \"round\": i,\n",
    "                \"epoch\": f\"{epoch+1}/{num_epochs}\",\n",
    "                \"loss\": f\"{epoch_loss:.4f}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    with open(\"result.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 평가\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # 데이터도 GPU로 이동\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    accuracis.append(f\"{accuracy:.2f}\")\n",
    "\n",
    "    with open(\"accuracis.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(accuracis, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    torch.save(model, PATH + f\"model.{i}.pt\")  # 전체 모델 저장\n",
    "    torch.save(\n",
    "        model.state_dict(), PATH + f\"model_state_dict.{i}.pt\"\n",
    "    )  # 모델 객체의 state_dict 저장\n",
    "    torch.save(\n",
    "        {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()},\n",
    "        PATH + f\"all.{i}.tar\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "models = [\n",
    "    joblib.load(\"models/dt_model_v1.joblib\"),\n",
    "    joblib.load(\"models/rf_model_v1.joblib\"),\n",
    "    joblib.load(\"models/knn_model_v1.joblib\"),\n",
    "    joblib.load(\"models/ensemble_model_v1.joblib\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8549\n",
      "Random Forest Accuracy: 0.7479\n",
      "KNN Accuracy: 0.2820\n",
      "Ensemble Accuracy: 0.9484\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    ensemble_pred = model.predict(g_x_test)\n",
    "    ensemble_acc = accuracy_score(g_y_test, ensemble_pred)\n",
    "    if i == 0:\n",
    "        print(f\"Decision Tree Accuracy: {ensemble_acc:.4f}\")\n",
    "    elif i == 1:\n",
    "        print(f\"Random Forest Accuracy: {ensemble_acc:.4f}\")\n",
    "    elif i == 2:\n",
    "        print(f\"KNN Accuracy: {ensemble_acc:.4f}\")\n",
    "    elif i == 3:\n",
    "        print(f\"Ensemble Accuracy: {ensemble_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
